\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}

\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\newcommand*{\permcomb}[4][0mu]{{{}^{#3}\mkern#1#2_{#4}}}
\newcommand{\perm}[1][-3mu]{\permcomb[#1]{P}}
\makeatother
\usepackage[hidelinks]{hyperref}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{fullpage}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{tabularx}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage[margin=0.7in]{geometry}    % For reducing margin
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{physics}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{comment}
\usepackage{framed}

\newcommand{\wx}[1]{\textcolor{magenta}{\bf\small [#1 --WX]}}

\begin{document}

\title{CS 7650: Natural Language Processing \\ Fall 2022 \\ Problem Set 1}
\author{Instructor: Dr. Wei Xu \\ TAs: Chase Perry, Rahul Katre, Rucha Sathe, Xurui Zhang \\Piazza: \url{https://piazza.com/class/l6vgipz0vsm1kk}
\\Gradescope: \url{https://gradescope.com/courses/418978}}
\date{Due: Tuesday, September 13, 11:59 PM ET}
\maketitle

    \section{Logistic vs Softmax}

    \begin{enumerate}
        \item (\textbf{2 pts}) Recall the Logistic and Softmax functions
    \begin{align*}
        P_{Logistic}(y=1|\mathbf{x}) = \frac{e^{\mathbf{w}^T\mathbf{x}}}{1 + e^{\mathbf{w}^T\mathbf{x}}}
    \end{align*}
    
    \begin{align*}
        P_{Softmax}(y|\mathbf{x}) = \frac{e^{\mathbf{w}_y^T\mathbf{x}}}{\sum_{y' \in \mathcal{Y}} e^{\mathbf{w}_{y'}^T\mathbf{x}}} \\
    \end{align*}
    Given $\mathcal{Y} = \{0,1\}$, what should be the value of $\mathbf{w}$ such that \\ $P_{Logistic}(y|\textbf{x}) = P_{Softmax}(y|\textbf{x}) \hspace{0.5em} \forall \hspace{0.5em} y \in \mathcal{Y}$? Show your work. \\
    Hint: Expand the summation term and think about $\mathbf{w}$ in terms of $\mathbf{w}_0$ and $\mathbf{w}_1$.
    
    \item (\textbf{2 pts}) Recall that the Softmax function is a generalization of the logistic sigmoid for multiclass classification. In practice, machine learning software such as PyTorch uses a Softmax implementation for both binary and multiclass classification. Also recall that the Softmax function produces a vector output $\mathbf{z} \in \mathbb{R}^{|\mathcal{Y}|}$ and the logistic function a single scalar value $z$, representing class probabilities. Write the equation for a decision rule to produce $\hat{y}$ from the Softmax function in the binary case (when $\mathcal{Y} = \{0,1\}$; you can break ties arbitrarily).
    Write the decision rule to produce $\hat{y}$ from the logistic function. Compare the two rules. How are they similar and/or different? (1-2 sentences).
    \end{enumerate}

\newpage
\section{Multiclass Naive Bayes with Bag of Words}

    A group of artists wishes to use Naive Bayes algorithm to classify a given artwork into three different categories based on the colors that have been used while making it. The three categories are related to the overall color scheme and are as follows: Warm, Cool and Neutral. A set of colors has been sampled to be observed in each artwork and classification has been done on the basis of the presence or absence of each color. The data collected so far is given in the table below:
    
    \begin{table}[h!]
    \centering
    \small
    \begin{tabular}{|l | r | r | r | r | r | r | r | r | l|}

    
    \hline
    S.No. & red & crimson & yellow & orange & purple & green & teal & blue & Y \\ \hline
    1           & 1           & 0         & 0    & 1     & 1       & 0       & 0            & 0      & Warm \\
    2           & 1           & 0         & 1    & 0     & 0       & 1       & 1            & 1      & Neutral\\
    3           & 0           & 0         & 1    & 1     & 0       & 1       & 1            & 1      & Cool\\
    4           & 0           & 1         & 1    & 1     & 1       & 1       & 0            & 0      & Neutral\\
    5           & 0           & 1         & 0    & 1     & 1       & 0       & 0            & 0      & Warm\\
    6           & 0           & 1         & 1    & 0     & 0       & 1       & 1            & 1      & Cool\\
    7           & 0           & 0         & 0    & 1     & 0       & 0       & 0            & 1      & Warm\\
    8           & 1           & 0         & 0    & 0     & 1       & 0       & 0            & 0      & Warm\\
    \hline
    \end{tabular}
    \end{table}
    
    \begin{enumerate}[label=\alph*.]
        \item (\textbf{1 pt}) What is the probability $\theta_y$ of each label $y \in$ \{Warm, Neutral, Cool\}?\\
        
        \item (\textbf{3 pts}) The parameter $\phi_{y, j}$ is the probability of a token $j$ appearing with label $y$. It is defined by the following equation, where $V$ is the size of the vocabulary set:
        $$ \phi_{y, j} = \frac{\text{count}(y, j)}{\sum^V_{j'=1}\text{count}(y, j')}$$
        
        The probability of a count of words $x$ and a label $y$ is defined as follows:
        $$ \mathrm{p}(x, y; \theta, \phi) = \mathrm{p}(y; \theta) \cdot \mathrm{p}(x|y; \phi) = \mathrm{p}(y; \theta)\prod^V_{j=1} \phi^{x_j}_{y, j} $$
        
        Find the most likely label $\hat y$ for the following word counts vector $x = (0,1,0,1,1,0,0,1)$ using $\hat y = \text{argmax}_y \log \mathrm{p}(x, y; \theta; \phi)$. Show final log (base-10) probabilities for each label rounded to 3 decimals. Treat $\log(0)$ as $-\infty$.
        
        \item (\textbf{3 pts}) When calculating $\text{argmax}_y$, if $\phi_{y, j} = 0$ for a label-word pair, the label $y$  is no longer considered. This is an issue, especially for smaller datasets where a feature may not be present in all documents for a certain label. One approach to mitigating this high variance is to smooth the probabilities. Using add-1 smoothing, which redefines $\phi_{y, j}$, again find the most likely label $\hat y$ for the following word counts vector $x = (0,1,0,1,1,0,0,1)$ using $\hat y = \text{argmax}_y \log \mathrm{p}(x, y; \theta; \phi)$. Make sure to show final log probabilities.\\
        $$\text{add-1 smoothing: } \phi_{y, j} = \frac{1 + \text{count}(y, j)}{V + \sum^V_{j'=1}\text{count}(y, j')}$$\\
    \end{enumerate}
    
\newpage
\section{Perceptron: Linear Separability and Weight Scaling}
\begin{enumerate}[label=(\alph*)]
\item (\textbf{2 pts}) Suppose we have the following data representing the XOR function:

    \begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline    
         $x_1$ & $x_2$ & $f(x_1,x_2)$ \\
        \hline
        0    & 0 & -1 \\
        0    & 1 & 1 \\
        1    & 0 & 1 \\
        1    & 1 & -1 \\        
        \hline
    \end{tabular}
    \caption{XOR function data}
    \label{tab:my_label}
    \end{table}
    
Evidently, the data is not linearly separable. Therefore the perceptron algorithm will not be able to learn a classifier for XOR, based on this data.

However, we can add a $3^{rd}$ dimension/feature to each input such that the data becomes linearly separable. If we add $(1, 0, 0, 1)$ to the $3^{rd}$ dimension ($x_3$) of the four data points in order, will the new data be linearly separable? Assume 0 is the threshold for classification. Justify your answer.

Does the ability to add a $3^{rd}$ dimension indicate that the perceptron algorithm is capable of learning the XOR function? Why or why not?

\bigskip

\item (\textbf{2 pts}) Suppose we have a trained Perceptron with parameters $(W, b)$. If we scale $W$ by a positive constant factor $c$, will the new set of weights produce the exact same prediction for all the test data? Assume the threshold for classification is 0. Justify your answer.

\bigskip

\item (\textbf{2 pts}) With the same setting as 2, this time we translate $W$ by a positive constant factor $c$ (add $c$ to each element of $W$), will the new set of weights produce the exact same prediction for all the test data? Justify your answer.\\
\bigskip 

\end{enumerate}

\bigskip

\newpage
\section{Feedforward Neural Network}
    
    [Eisenstein Chapter 3 Problem 4] (\textbf{2 pts}) In Question 3, we tried to design a perceptron architecture in order to learn the XOR function represented by Table \ref{tab:my_label}. Now, we want you to design a feedforward neural network to compute the XOR function. This can be done in several different ways, so make sure you provide ample description of your design!\\
    
    \noindent Use a single output node and \textbf{specify} the activation function you choose for it. Also use a single hidden layer with ReLU activation function.  Describe all weights and offsets (bias terms) and ensure you design your network in accordance with Table \ref{tab:my_label}.\\
    
    \noindent \textit{(Hint: In class, we discussed a neural network design that solves the XOR problem using \textbf{tanh} activation functions.)}
    
    \newpage
    \section{Dead Neurons}
        
    [Eisenstein Chapter 3 Problem 8] The ReLU activation function can lead to ``dead neurons'', which can never be activated on any input. Consider a feedforward neural network with a single hidden layer and ReLU nonlinearity, assuming a binary input vector, $\mathbf{x} \in f\{0,1\}^D$  and scalar output $y$:
    \begin{center}
    $z_i = \text{ReLU}(\mathbf{\theta}_i^{(x \rightarrow z)} \cdot \mathbf{x} + b_i)$ \\
    $\mathbf{y} = \mathbf{\theta}^{(z \rightarrow y)} \cdot \mathbf{z}$
    \end{center}
    
    Assume the above function is optimized to minimze a loss function (e.g., mean squared error) using stochastic gradient descent. 
    
    \begin{enumerate}
        \item (\textbf{2 pts}) Under what condition is node $z_i$ ``dead''? Your answer should be expressed in terms of the parameters $\mathbf{\theta}_i^{(x \rightarrow z)}$ and $b_i$.\\\\
        
        \item (\textbf{2 pts}) Suppose that the gradient of the loss on a given instance is $\frac{\partial l}{\partial y} = 1$. Derive gradients $\frac{\partial l}{\partial b_i}$ and $\frac{\partial l}{\partial \mathbf{\theta}_{j,i}^{(x \rightarrow z)}}$ for such an instance.\\\\
        
        \item (\textbf{2 pts}) Using your answers to the previous two parts, explain why a ``dead'' neuron can never be brought back to life during gradient-based learning. \\\\
    
    \end{enumerate}

\end{document}